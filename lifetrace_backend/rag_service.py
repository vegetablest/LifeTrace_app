import asyncio
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Generator, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œä»¥ä¾¿ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶
if __name__ == "__main__":
    project_root = Path(__file__).parent.parent
    sys.path.insert(0, str(project_root))

from lifetrace_backend.context_builder import ContextBuilder
from lifetrace_backend.llm_client import LLMClient
from lifetrace_backend.query_parser import QueryConditions, QueryParser
from lifetrace_backend.retrieval_service import RetrievalService
from lifetrace_backend.storage import DatabaseManager

logger = logging.getLogger(__name__)


class RAGService:
    """RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ) æœåŠ¡ï¼Œæ•´åˆæŸ¥è¯¢è§£æã€æ•°æ®æ£€ç´¢ã€ä¸Šä¸‹æ–‡æ„å»ºå’ŒLLMç”Ÿæˆ"""

    def __init__(
        self,
        db_manager: DatabaseManager,
        api_key: str = None,
        base_url: str = None,
        model: str = None,
    ):
        """
        åˆå§‹åŒ–RAGæœåŠ¡

        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
            api_key: LLM APIå¯†é’¥
            base_url: LLM APIåŸºç¡€URL
            model: LLMæ¨¡å‹åç§°
        """
        self.db_manager = db_manager
        self.llm_client = LLMClient(api_key, base_url, model)
        self.retrieval_service = RetrievalService(db_manager)
        self.context_builder = ContextBuilder()
        self.query_parser = QueryParser(self.llm_client)

        logger.info("RAGæœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    async def process_query(
        self, user_query: str, max_results: int = 50
    ) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„å®Œæ•´RAGæµæ°´çº¿

        Args:
            user_query: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            max_results: æœ€å¤§æ£€ç´¢ç»“æœæ•°é‡

        Returns:
            åŒ…å«ç”Ÿæˆç»“æœå’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
        """
        start_time = datetime.now()

        try:
            # 1. æ„å›¾è¯†åˆ«
            logger.info(f"å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_query}")
            intent_result = self.llm_client.classify_intent(user_query)

            # å¦‚æœä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢ï¼Œç›´æ¥ä½¿ç”¨LLMç”Ÿæˆå›å¤
            if not intent_result.get("needs_database", True):
                logger.info(f"ç”¨æˆ·æ„å›¾ä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢: {intent_result['intent_type']}")
                if self.llm_client.is_available():
                    response_text = self._generate_direct_response(
                        user_query, intent_result
                    )
                else:
                    response_text = self._fallback_direct_response(
                        user_query, intent_result
                    )

                processing_time = (datetime.now() - start_time).total_seconds()
                return {
                    "success": True,
                    "response": response_text,
                    "query_info": {
                        "original_query": user_query,
                        "intent_classification": intent_result,
                        "requires_database": False,
                    },
                    "performance": {
                        "processing_time_seconds": processing_time,
                        "timestamp": start_time.isoformat(),
                    },
                }

            # 2. æŸ¥è¯¢è§£æï¼ˆä»…å½“éœ€è¦æ•°æ®åº“æŸ¥è¯¢æ—¶ï¼‰
            logger.info("éœ€è¦æ•°æ®åº“æŸ¥è¯¢ï¼Œå¼€å§‹æŸ¥è¯¢è§£æ")
            parsed_query = self.query_parser.parse_query(user_query)
            # ç¡®å®šæŸ¥è¯¢ç±»å‹
            query_type = "statistics" if "ç»Ÿè®¡" in user_query else "search"

            # 3. æ•°æ®æ£€ç´¢ - ä½¿ç”¨å·²è§£æçš„æŸ¥è¯¢æ¡ä»¶ï¼Œé¿å…é‡å¤è§£æ
            logger.info("å¼€å§‹æ•°æ®æ£€ç´¢")
            print(parsed_query)

            retrieved_data = self.retrieval_service.search_by_conditions(
                parsed_query, max_results
            )

            # 4. è·å–ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
            stats = None
            if query_type == "statistics" or "ç»Ÿè®¡" in user_query:
                # å®‰å…¨åœ°è®¿é—®parsed_queryçš„å±æ€§
                if isinstance(parsed_query, QueryConditions):
                    start_date = parsed_query.start_date
                    end_date = parsed_query.end_date
                    app_names = parsed_query.app_names
                    keywords = parsed_query.keywords or []
                else:
                    # å¦‚æœparsed_queryæ˜¯å­—å…¸ï¼Œä»å­—å…¸ä¸­è·å–å€¼
                    start_date = parsed_query.get("start_date")
                    end_date = parsed_query.get("end_date")
                    app_names = parsed_query.get("app_names", [])
                    keywords = parsed_query.get("keywords", [])

                conditions = QueryConditions(
                    start_date=start_date,
                    end_date=end_date,
                    app_names=app_names,
                    keywords=keywords,
                )
                stats = self.retrieval_service.get_statistics(conditions)

            # 5. ä¸Šä¸‹æ–‡æ„å»º
            logger.info("å¼€å§‹æ„å»ºä¸Šä¸‹æ–‡")
            if query_type == "statistics":
                context_text = self.context_builder.build_statistics_context(
                    user_query, retrieved_data, stats
                )
            elif query_type == "search":
                context_text = self.context_builder.build_search_context(
                    user_query, retrieved_data
                )
            else:
                context_text = self.context_builder.build_summary_context(
                    user_query, retrieved_data
                )

            # 6. LLMç”Ÿæˆ
            logger.info("å¼€å§‹LLMç”Ÿæˆ")
            if self.llm_client.is_available():
                response_text = self.llm_client.generate_summary(
                    user_query, retrieved_data
                )
            else:
                response_text = self._fallback_response(
                    user_query, retrieved_data, stats
                )

            # 7. æ„å»ºå“åº”
            processing_time = (datetime.now() - start_time).total_seconds()

            result = {
                "success": True,
                "response": response_text,
                "query_info": {
                    "original_query": user_query,
                    "intent_classification": intent_result,
                    "parsed_query": parsed_query,
                    "query_type": query_type,
                    "requires_database": True,
                },
                "retrieval_info": {
                    "total_found": len(retrieved_data),
                    "data_summary": self._summarize_retrieved_data(retrieved_data),
                },
                "context_info": {
                    "context_length": len(context_text),
                    "llm_available": self.llm_client.is_available(),
                },
                "performance": {
                    "processing_time_seconds": processing_time,
                    "timestamp": start_time.isoformat(),
                },
                "statistics": stats,
            }

            logger.info(f"æŸ¥è¯¢å¤„ç†å®Œæˆï¼Œè€—æ—¶ {processing_time:.2f} ç§’")
            return result

        except Exception as e:
            logger.error(f"RAGæŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            # å®‰å…¨åœ°æ„å»ºé”™è¯¯ä¿¡æ¯
            error_query_info = {"original_query": user_query}
            try:
                if "parsed_query" in locals():
                    error_query_info["error"] = str(e)
            except:  # noqa: E722
                pass

            return {
                "success": False,
                "error": str(e),
                "response": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‡ºç°äº†é”™è¯¯ã€‚è¯·ç¨åé‡è¯•ã€‚",
                "query_info": error_query_info,
                "performance": {
                    "processing_time_seconds": (
                        datetime.now() - start_time
                    ).total_seconds(),
                    "timestamp": start_time.isoformat(),
                },
            }

    def process_query_sync(
        self, user_query: str, max_results: int = 50
    ) -> Dict[str, Any]:
        """
        åŒæ­¥ç‰ˆæœ¬çš„æŸ¥è¯¢å¤„ç†

        Args:
            user_query: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            max_results: æœ€å¤§æ£€ç´¢ç»“æœæ•°é‡

        Returns:
            åŒ…å«ç”Ÿæˆç»“æœå’Œç›¸å…³ä¿¡æ¯çš„å­—å…¸
        """
        return asyncio.run(self.process_query(user_query, max_results))

    def post_stream_decision(self, user_query: str, output_text: str) -> None:
        """
        æµå¼è¾“å‡ºå®Œæˆåçš„åˆ¤å®š/è®°å½•é’©å­ï¼š
        - ç”¨äºæ‰§è¡Œé‚£äº›â€œå¿…é¡»æ‹¿åˆ°å®Œæ•´è¾“å‡ºæ‰èƒ½åˆ¤æ–­â€çš„é€»è¾‘ï¼ˆä¾‹å¦‚ï¼šæ˜¯å¦éœ€è¦è¿½åŠ å…è´£å£°æ˜ã€æ˜¯å¦è§¦å‘æŸäº›åç»­åŠ¨ä½œç­‰ï¼‰
        - é»˜è®¤å®ç°ä»…åšæ—¥å¿—è®°å½•ï¼Œåç»­å¯æŒ‰éœ€æ‰©å±•
        """
        try:
            if not output_text:
                return
            # ç¤ºä¾‹ï¼šå¦‚æœè¾“å‡ºåŒ…å«ç‰¹å®šæç¤ºè¯ï¼Œåˆ™è®°å½•åˆ°æ—¥å¿—æˆ–è§¦å‘åç»­å¤„ç†
            keywords = ["å…è´£å£°æ˜", "æ•æ„Ÿå†…å®¹", "æ³¨æ„", "æ€»ç»“"]
            if any(kw in output_text for kw in keywords):
                logger.info(
                    f"[post_stream] è¾“å‡ºåŒ…å«å…³é”®æç¤ºï¼Œquery='{user_query[:50]}...' è§¦å‘æ ‡è®°"
                )
            else:
                logger.debug("[post_stream] æ— ç‰¹æ®Šæ ‡è®°")
        except Exception as e:
            logger.debug(f"[post_stream] å¤„ç†å¼‚å¸¸å·²å¿½ç•¥: {e}")

    def stream_query(
        self,
        user_query: str,
        max_results: int = 50,
        temperature_direct: float = 0.7,
        temperature_rag: float = 0.3,
    ) -> Generator[str, None, None]:
        """
        æµå¼å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼šæ‰§è¡Œå®Œæ•´çš„RAGæµç¨‹ï¼Œå¹¶åœ¨ç”Ÿæˆé˜¶æ®µé€tokenï¼ˆæˆ–é€chunkï¼‰yield æ–‡æœ¬ã€‚
        å½“åº•å±‚LLMä¸æ”¯æŒçœŸæµå¼æ—¶ï¼Œå°†æŒ‰æ®µè¿”å›ï¼›å½“ä¸å¯ç”¨æ—¶ï¼Œè¿”å›å¤‡ç”¨æ–‡æœ¬ã€‚
        åœ¨æµå¼è¾“å‡ºå®Œæˆåï¼Œè°ƒç”¨ post_stream_decision è¿›è¡Œåç»­åˆ¤å®š/è®°å½•ã€‚
        """
        try:
            # 1) æ„å›¾è¯†åˆ«
            intent_result = self.llm_client.classify_intent(user_query)
            needs_db = intent_result.get("needs_database", True)

            # 2) ä¸éœ€è¦æ•°æ®åº“ï¼šç›´æ¥å¯¹è¯
            if not needs_db:
                if not self.llm_client.is_available():
                    # LLMä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›å¤‡ç”¨æ–‡æœ¬
                    fallback_text = self._fallback_direct_response(
                        user_query, intent_result
                    )
                    yield fallback_text
                    # å®Œæ•´è¾“å‡ºåå¤„ç†
                    self.post_stream_decision(user_query, fallback_text)
                    return
                # ç³»ç»Ÿæç¤ºä¸ _generate_direct_response ä¿æŒä¸€è‡´
                intent_type = intent_result.get("intent_type", "general_chat")
                if intent_type == "system_help":
                    system_prompt = """
ä½ æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ã€‚LifeTraceæ˜¯ä¸€ä¸ªç”Ÿæ´»è½¨è¿¹è®°å½•å’Œåˆ†æç³»ç»Ÿï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. è‡ªåŠ¨æˆªå›¾è®°å½•ç”¨æˆ·çš„å±å¹•æ´»åŠ¨
2. OCRæ–‡å­—è¯†åˆ«å’Œå†…å®¹åˆ†æ
3. åº”ç”¨ä½¿ç”¨æƒ…å†µç»Ÿè®¡
4. æ™ºèƒ½æœç´¢å’ŒæŸ¥è¯¢åŠŸèƒ½

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›æœ‰ç”¨çš„å¸®åŠ©ä¿¡æ¯ã€‚
"""
                else:
                    system_prompt = """
ä½ æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ä»¥å‹å¥½ã€è‡ªç„¶çš„æ–¹å¼ä¸ç”¨æˆ·å¯¹è¯ã€‚
å¦‚æœç”¨æˆ·éœ€è¦æŸ¥è¯¢æ•°æ®æˆ–ç»Ÿè®¡ä¿¡æ¯ï¼Œè¯·å¼•å¯¼ä»–ä»¬ä½¿ç”¨å…·ä½“çš„æŸ¥è¯¢è¯­å¥ã€‚
"""
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_query},
                ]
                output_chunks: List[str] = []
                for text in self.llm_client.stream_chat(
                    messages=messages, temperature=temperature_direct
                ):
                    if text:
                        output_chunks.append(text)
                        yield text
                # å®Œæ•´è¾“å‡ºåå¤„ç†
                self.post_stream_decision(user_query, "".join(output_chunks))
                return

            # 3) éœ€è¦æ•°æ®åº“ï¼šè§£æ + æ£€ç´¢ + æ„å»ºä¸Šä¸‹æ–‡
            parsed_query = self.query_parser.parse_query(user_query)
            query_type = "statistics" if "ç»Ÿè®¡" in user_query else "search"
            retrieved_data = self.retrieval_service.search_by_conditions(
                parsed_query, max_results
            )

            stats = None
            if query_type == "statistics" or "ç»Ÿè®¡" in user_query:
                # å…¼å®¹ QueryConditions æˆ– dict
                if isinstance(parsed_query, QueryConditions):
                    conditions = parsed_query
                else:
                    conditions = QueryConditions(
                        start_date=parsed_query.get("start_date"),
                        end_date=parsed_query.get("end_date"),
                        app_names=parsed_query.get("app_names"),
                        keywords=parsed_query.get("keywords", []),
                    )
                try:
                    stats = self.retrieval_service.get_statistics(conditions)
                except Exception:
                    stats = None

            # ä¸Šä¸‹æ–‡æ„å»º
            if query_type == "statistics":
                context_text = self.context_builder.build_statistics_context(
                    user_query, retrieved_data, stats
                )
            elif query_type == "search":
                context_text = self.context_builder.build_search_context(
                    user_query, retrieved_data
                )
            else:
                context_text = self.context_builder.build_summary_context(
                    user_query, retrieved_data
                )

            # LLM ä¸å¯ç”¨æ—¶ï¼Œè¿”å›è§„åˆ™å¤‡é€‰
            if not self.llm_client.is_available():
                fallback_text = self._fallback_response(
                    user_query, retrieved_data, stats
                )
                yield fallback_text
                # å®Œæ•´è¾“å‡ºåå¤„ç†
                self.post_stream_decision(user_query, fallback_text)
                return

            # 4) ç”Ÿæˆé˜¶æ®µï¼šæµå¼è¾“å‡º
            system_prompt = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·åˆ†æå’Œæ€»ç»“å†å²è®°å½•æ•°æ®ã€‚

ç”¨æˆ·ä¼šæä¾›ä¸€ä¸ªæŸ¥è¯¢å’Œç›¸å…³çš„å†å²æ•°æ®ï¼Œä½ éœ€è¦ï¼š
1. ç†è§£ç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾
2. åˆ†ææä¾›çš„å†å²æ•°æ®
3. ç”Ÿæˆå‡†ç¡®ã€æœ‰ç”¨çš„æ€»ç»“

**å¼ºåˆ¶æ€§è¦æ±‚ - å¿…é¡»ä¸¥æ ¼éµå®ˆï¼š**
- æ¯å½“å¼•ç”¨æˆ–æåˆ°ä»»ä½•å…·ä½“ä¿¡æ¯æ—¶ï¼Œå¿…é¡»æ ‡æ³¨æˆªå›¾IDæ¥æºï¼Œæ ¼å¼ä¸ºï¼š[æˆªå›¾ID: xxx]
- ä¸å…è®¸æåŠä»»ä½•ä¿¡æ¯è€Œä¸æ ‡æ³¨å…¶æ¥æºæˆªå›¾ID
- å¦‚æœå†å²æ•°æ®ä¸­åŒ…å«æˆªå›¾IDä¿¡æ¯ï¼Œå¿…é¡»åœ¨ç›¸å…³å†…å®¹åç«‹å³æ·»åŠ å¼•ç”¨
- è¿™æ˜¯ä¸ºäº†ç¡®ä¿ä¿¡æ¯çš„å¯è¿½æº¯æ€§å’Œå‡†ç¡®æ€§
- ç¤ºä¾‹ï¼š"ç”¨æˆ·åœ¨å¾®ä¿¡ä¸­å‘é€äº†æ¶ˆæ¯ [æˆªå›¾ID: 12345]"

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä¿æŒç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡ºå…³é”®ä¿¡æ¯ã€‚
"""
            user_prompt = f"""
ç”¨æˆ·æŸ¥è¯¢ï¼š{user_query}

ç›¸å…³å†å²æ•°æ®ï¼š
{context_text}

è¯·åŸºäºä»¥ä¸Šæ•°æ®å›ç­”ç”¨æˆ·çš„æŸ¥è¯¢ã€‚
"""
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ]
            output_chunks: List[str] = []
            for text in self.llm_client.stream_chat(
                messages=messages, temperature=temperature_rag
            ):
                if text:
                    output_chunks.append(text)
                    yield text
            # å®Œæ•´è¾“å‡ºåå¤„ç†
            self.post_stream_decision(user_query, "".join(output_chunks))
        except Exception as e:
            logger.error(f"RAG æµå¼å¤„ç†å¤±è´¥: {e}")
            error_text = "\n[æç¤º] æµå¼å¤„ç†å‡ºç°å¼‚å¸¸ï¼Œå·²ç»“æŸã€‚"
            yield error_text
            # å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿåšä¸€æ¬¡åå¤„ç†
            try:
                self.post_stream_decision(user_query, error_text)
            except Exception:
                pass

    def get_query_suggestions(self, partial_query: str = "") -> List[str]:
        """
        è·å–æŸ¥è¯¢å»ºè®®

        Args:
            partial_query: éƒ¨åˆ†æŸ¥è¯¢æ–‡æœ¬

        Returns:
            æŸ¥è¯¢å»ºè®®åˆ—è¡¨
        """
        suggestions = [
            "æ€»ç»“ä»Šå¤©çš„å¾®ä¿¡èŠå¤©è®°å½•",
            "æŸ¥æ‰¾åŒ…å«'ä¼šè®®'çš„æ‰€æœ‰è®°å½•",
            "ç»Ÿè®¡æœ€è¿‘ä¸€å‘¨å„åº”ç”¨çš„ä½¿ç”¨æƒ…å†µ",
            "æœç´¢æ˜¨å¤©æµè§ˆå™¨ä¸­çš„å†…å®¹",
            "æ€»ç»“æœ€è¿‘çš„å·¥ä½œç›¸å…³æˆªå›¾",
            "æŸ¥æ‰¾åŒ…å«'é¡¹ç›®'å…³é”®è¯çš„è®°å½•",
            "ç»Ÿè®¡æœ¬æœˆQQèŠå¤©è®°å½•æ•°é‡",
            "æœç´¢æœ€è¿‘3å¤©çš„å­¦ä¹ èµ„æ–™",
            "æ€»ç»“ä¸Šå‘¨çš„ç½‘é¡µæµè§ˆè®°å½•",
            "æŸ¥æ‰¾åŒ…å«'æ–‡æ¡£'çš„æ‰€æœ‰åº”ç”¨è®°å½•",
        ]

        if partial_query:
            # ç®€å•çš„æ¨¡ç³ŠåŒ¹é…
            filtered_suggestions = [
                s
                for s in suggestions
                if any(word in s for word in partial_query.split())
            ]
            return filtered_suggestions[:5]

        return suggestions[:5]

    def get_supported_query_types(self) -> Dict[str, Any]:
        """
        è·å–æ”¯æŒçš„æŸ¥è¯¢ç±»å‹ä¿¡æ¯

        Returns:
            æŸ¥è¯¢ç±»å‹ä¿¡æ¯å­—å…¸
        """
        return {
            "query_types": {
                "summary": {
                    "name": "æ€»ç»“",
                    "description": "å¯¹å†å²è®°å½•è¿›è¡Œæ€»ç»“å’Œæ¦‚æ‹¬",
                    "examples": ["æ€»ç»“ä»Šå¤©çš„å¾®ä¿¡èŠå¤©", "æ¦‚æ‹¬æœ€è¿‘çš„å·¥ä½œè®°å½•"],
                },
                "search": {
                    "name": "æœç´¢",
                    "description": "æœç´¢åŒ…å«ç‰¹å®šå…³é”®è¯çš„è®°å½•",
                    "examples": ["æŸ¥æ‰¾åŒ…å«'ä¼šè®®'çš„è®°å½•", "æœç´¢é¡¹ç›®ç›¸å…³å†…å®¹"],
                },
                "statistics": {
                    "name": "ç»Ÿè®¡",
                    "description": "ç»Ÿè®¡å’Œåˆ†æå†å²è®°å½•æ•°æ®",
                    "examples": ["ç»Ÿè®¡å„åº”ç”¨ä½¿ç”¨æƒ…å†µ", "åˆ†ææœ€è¿‘ä¸€å‘¨çš„æ´»åŠ¨"],
                },
            },
            "supported_apps": [
                "WeChat",
                "QQ",
                "Browser",
                "Chrome",
                "Firefox",
                "Edge",
                "Word",
                "Excel",
                "PowerPoint",
                "Notepad",
                "VSCode",
            ],
            "time_expressions": [
                "ä»Šå¤©",
                "æ˜¨å¤©",
                "æœ€è¿‘3å¤©",
                "æœ¬å‘¨",
                "ä¸Šå‘¨",
                "æœ¬æœˆ",
                "ä¸Šæœˆ",
            ],
        }

    def _summarize_retrieved_data(
        self, retrieved_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """æ€»ç»“æ£€ç´¢åˆ°çš„æ•°æ®"""
        if not retrieved_data:
            return {"apps": {}, "time_range": None, "total": 0}

        app_counts = {}
        timestamps = []

        for record in retrieved_data:
            app_name = record.get("app_name", "æœªçŸ¥åº”ç”¨")
            app_counts[app_name] = app_counts.get(app_name, 0) + 1

            timestamp = record.get("timestamp")
            if timestamp:
                timestamps.append(timestamp)

        time_range = None
        if timestamps:
            timestamps.sort()
            time_range = {"earliest": timestamps[0], "latest": timestamps[-1]}

        return {
            "apps": app_counts,
            "time_range": time_range,
            "total": len(retrieved_data),
        }

    def _fallback_response(
        self,
        user_query: str,
        retrieved_data: List[Dict[str, Any]],
        stats: Dict[str, Any] = None,
    ) -> str:
        """
        å¤‡ç”¨å“åº”ç”Ÿæˆï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            retrieved_data: æ£€ç´¢åˆ°çš„æ•°æ®
            stats: ç»Ÿè®¡ä¿¡æ¯

        Returns:
            å¤‡ç”¨å“åº”æ–‡æœ¬
        """
        if not retrieved_data:
            return f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸æŸ¥è¯¢ '{user_query}' ç›¸å…³çš„å†å²è®°å½•ã€‚"

        response_parts = [f"æ ¹æ®æ‚¨çš„æŸ¥è¯¢ '{user_query}'ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ä¿¡æ¯ï¼š", ""]

        # åŸºç¡€ç»Ÿè®¡
        response_parts.append(f"ğŸ“Š æ€»å…±æ‰¾åˆ° {len(retrieved_data)} æ¡ç›¸å…³è®°å½•")

        # åº”ç”¨åˆ†å¸ƒ
        app_summary = self._summarize_retrieved_data(retrieved_data)
        if app_summary["apps"]:
            response_parts.append("\nğŸ“± åº”ç”¨åˆ†å¸ƒï¼š")
            for app, count in sorted(
                app_summary["apps"].items(), key=lambda x: x[1], reverse=True
            ):
                response_parts.append(f"  â€¢ {app}: {count} æ¡è®°å½•")

        # æ—¶é—´èŒƒå›´
        if app_summary["time_range"]:
            try:
                earliest = datetime.fromisoformat(
                    app_summary["time_range"]["earliest"].replace("Z", "+00:00")
                )
                latest = datetime.fromisoformat(
                    app_summary["time_range"]["latest"].replace("Z", "+00:00")
                )
                response_parts.append(
                    f"\nâ° æ—¶é—´èŒƒå›´: {earliest.strftime('%Y-%m-%d %H:%M')} è‡³ {latest.strftime('%Y-%m-%d %H:%M')}"
                )
            except:  # noqa: E722
                pass

        # æœ€æ–°è®°å½•ç¤ºä¾‹
        if retrieved_data:
            response_parts.append("\nğŸ“ æœ€æ–°è®°å½•ç¤ºä¾‹ï¼š")
            latest_record = retrieved_data[0]
            timestamp = latest_record.get("timestamp", "æœªçŸ¥æ—¶é—´")
            app_name = latest_record.get("app_name", "æœªçŸ¥åº”ç”¨")
            ocr_text = latest_record.get("ocr_text", "æ— å†…å®¹")[:100]

            response_parts.append(f"  æ—¶é—´: {timestamp}")
            response_parts.append(f"  åº”ç”¨: {app_name}")
            response_parts.append(f"  å†…å®¹: {ocr_text}...")

        response_parts.append("\nğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯æ¥è·å¾—æ›´ç²¾ç¡®çš„ç»“æœã€‚")

        return "\n".join(response_parts)

    def health_check(self) -> Dict[str, Any]:
        """
        å¥åº·æ£€æŸ¥

        Returns:
            æœåŠ¡çŠ¶æ€ä¿¡æ¯
        """
        return {
            "rag_service": "healthy",
            "llm_client": (
                "available" if self.llm_client.is_available() else "unavailable"
            ),
            "database": "connected" if self.db_manager else "disconnected",
            "components": {
                "retrieval_service": "ready",
                "context_builder": "ready",
                "query_parser": "ready",
            },
            "timestamp": datetime.now().isoformat(),
        }

    def _generate_direct_response(
        self, user_query: str, intent_result: Dict[str, Any]
    ) -> str:
        """
        ä¸ºä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢çš„ç”¨æˆ·è¾“å…¥ç”Ÿæˆç›´æ¥å›å¤

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            intent_result: æ„å›¾è¯†åˆ«ç»“æœ

        Returns:
            ç”Ÿæˆçš„å›å¤æ–‡æœ¬
        """
        try:
            intent_type = intent_result.get("intent_type", "general_chat")

            if intent_type == "system_help":
                system_prompt = """
ä½ æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ã€‚LifeTraceæ˜¯ä¸€ä¸ªç”Ÿæ´»è½¨è¿¹è®°å½•å’Œåˆ†æç³»ç»Ÿï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. è‡ªåŠ¨æˆªå›¾è®°å½•ç”¨æˆ·çš„å±å¹•æ´»åŠ¨
2. OCRæ–‡å­—è¯†åˆ«å’Œå†…å®¹åˆ†æ
3. åº”ç”¨ä½¿ç”¨æƒ…å†µç»Ÿè®¡
4. æ™ºèƒ½æœç´¢å’ŒæŸ¥è¯¢åŠŸèƒ½

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›æœ‰ç”¨çš„å¸®åŠ©ä¿¡æ¯ã€‚
"""
            else:
                system_prompt = """
ä½ æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ä»¥å‹å¥½ã€è‡ªç„¶çš„æ–¹å¼ä¸ç”¨æˆ·å¯¹è¯ã€‚
å¦‚æœç”¨æˆ·éœ€è¦æŸ¥è¯¢æ•°æ®æˆ–ç»Ÿè®¡ä¿¡æ¯ï¼Œè¯·å¼•å¯¼ä»–ä»¬ä½¿ç”¨å…·ä½“çš„æŸ¥è¯¢è¯­å¥ã€‚
"""

            response = self.llm_client.client.chat.completions.create(
                model=self.llm_client.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_query},
                ],
                temperature=0.7,
                max_tokens=500,
            )

            # è®°å½•LLMå“åº”åˆ°æ§åˆ¶å°å’Œæ—¥å¿—
            llm_response = response.choices[0].message.content.strip()
            print(f"[LLM Direct Response] {llm_response}")
            logger.info(f"LLMç›´æ¥å“åº”: {llm_response}")

            return llm_response

        except Exception as e:
            logger.error(f"ç›´æ¥å“åº”ç”Ÿæˆå¤±è´¥: {e}")
            return self._fallback_direct_response(user_query, intent_result)

    def _fallback_direct_response(
        self, user_query: str, intent_result: Dict[str, Any]
    ) -> str:
        """
        å½“LLMä¸å¯ç”¨æ—¶çš„ç›´æ¥å›å¤å¤‡ç”¨æ–¹æ¡ˆ

        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            intent_result: æ„å›¾è¯†åˆ«ç»“æœ

        Returns:
            å¤‡ç”¨å›å¤æ–‡æœ¬
        """
        intent_type = intent_result.get("intent_type", "general_chat")

        if intent_type == "system_help":
            return """
LifeTraceæ˜¯ä¸€ä¸ªç”Ÿæ´»è½¨è¿¹è®°å½•å’Œåˆ†æç³»ç»Ÿï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š

ğŸ“¸ **è‡ªåŠ¨æˆªå›¾è®°å½•**
- å®šæœŸæ•è·å±å¹•å†…å®¹
- è®°å½•åº”ç”¨ä½¿ç”¨æƒ…å†µ

ğŸ” **æ™ºèƒ½æœç´¢**
- æœç´¢å†å²æˆªå›¾
- åŸºäºOCRæ–‡å­—å†…å®¹æŸ¥æ‰¾

ğŸ“Š **ä½¿ç”¨ç»Ÿè®¡**
- åº”ç”¨ä½¿ç”¨æ—¶é•¿ç»Ÿè®¡
- æ´»åŠ¨æ¨¡å¼åˆ†æ

ğŸ’¬ **æ™ºèƒ½é—®ç­”**
- è‡ªç„¶è¯­è¨€æŸ¥è¯¢
- ä¸ªæ€§åŒ–æ•°æ®åˆ†æ

å¦‚éœ€æŸ¥è¯¢å…·ä½“æ•°æ®ï¼Œè¯·ä½¿ç”¨å¦‚"æœç´¢åŒ…å«ç¼–ç¨‹çš„æˆªå›¾"æˆ–"ç»Ÿè®¡æœ€è¿‘ä¸€å‘¨çš„åº”ç”¨ä½¿ç”¨æƒ…å†µ"ç­‰è¯­å¥ã€‚
"""
        elif intent_type == "general_chat":
            greetings = [
                "ä½ å¥½ï¼æˆ‘æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼",
                "æ‚¨å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ",
                "æ¬¢è¿ä½¿ç”¨LifeTraceï¼æˆ‘å¯ä»¥å¸®æ‚¨æŸ¥è¯¢å’Œåˆ†ææ‚¨çš„ç”Ÿæ´»è½¨è¿¹æ•°æ®ã€‚",
            ]

            if any(word in user_query.lower() for word in ["ä½ å¥½", "hello", "hi"]):
                return (
                    greetings[0]
                    + "\n\næ‚¨å¯ä»¥è¯¢é—®æˆ‘å…³äºLifeTraceçš„åŠŸèƒ½ï¼Œæˆ–è€…ç›´æ¥æŸ¥è¯¢æ‚¨çš„æ•°æ®ã€‚"
                )
            elif any(word in user_query.lower() for word in ["è°¢è°¢", "thanks"]):
                return "ä¸å®¢æ°”ï¼å¦‚æœè¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘ã€‚"
            else:
                return (
                    greetings[1]
                    + "\n\næ‚¨å¯ä»¥å°è¯•æœç´¢æˆªå›¾ã€æŸ¥è¯¢åº”ç”¨ä½¿ç”¨æƒ…å†µï¼Œæˆ–è€…è¯¢é—®ç³»ç»ŸåŠŸèƒ½ã€‚"
                )
        else:
            return "æˆ‘ç†è§£æ‚¨çš„é—®é¢˜ï¼Œä½†å¯èƒ½éœ€è¦æ›´å¤šä¿¡æ¯æ‰èƒ½æä¾›å‡†ç¡®çš„å›ç­”ã€‚æ‚¨å¯ä»¥å°è¯•æ›´å…·ä½“çš„æŸ¥è¯¢ï¼Œæ¯”å¦‚æœç´¢ç‰¹å®šå†…å®¹æˆ–ç»Ÿè®¡ä½¿ç”¨æƒ…å†µã€‚"

    async def process_query_stream(self, user_query: str) -> Dict[str, Any]:
        """
        ä¸ºæµå¼æ¥å£å¤„ç†æŸ¥è¯¢ï¼Œè¿”å›æ„å»ºå¥½çš„messageså’Œtemperature
        é¿å…é‡å¤çš„æ„å›¾è¯†åˆ«è°ƒç”¨
        """
        try:
            # 1. æ„å›¾è¯†åˆ«
            logger.info(f"[stream] å¼€å§‹å¤„ç†æŸ¥è¯¢: {user_query}")
            intent_result = self.llm_client.classify_intent(user_query)
            needs_db = intent_result.get("needs_database", True)

            messages = []
            temperature = 0.7

            if not needs_db:
                # ä¸éœ€è¦æ•°æ®åº“æŸ¥è¯¢çš„æƒ…å†µ
                intent_type = intent_result.get("intent_type", "general_chat")
                if intent_type == "system_help":
                    system_prompt = """
ä½ æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ã€‚LifeTraceæ˜¯ä¸€ä¸ªç”Ÿæ´»è½¨è¿¹è®°å½•å’Œåˆ†æç³»ç»Ÿï¼Œä¸»è¦åŠŸèƒ½åŒ…æ‹¬ï¼š
1. è‡ªåŠ¨æˆªå›¾è®°å½•ç”¨æˆ·çš„å±å¹•æ´»åŠ¨
2. OCRæ–‡å­—è¯†åˆ«å’Œå†…å®¹åˆ†æ
3. åº”ç”¨ä½¿ç”¨æƒ…å†µç»Ÿè®¡
4. æ™ºèƒ½æœç´¢å’ŒæŸ¥è¯¢åŠŸèƒ½

è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›æœ‰ç”¨çš„å¸®åŠ©ä¿¡æ¯ã€‚
"""
                else:
                    system_prompt = """
ä½ æ˜¯LifeTraceçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ä»¥å‹å¥½ã€è‡ªç„¶çš„æ–¹å¼ä¸ç”¨æˆ·å¯¹è¯ã€‚
å¦‚æœç”¨æˆ·éœ€è¦æŸ¥è¯¢æ•°æ®æˆ–ç»Ÿè®¡ä¿¡æ¯ï¼Œè¯·å¼•å¯¼ä»–ä»¬ä½¿ç”¨å…·ä½“çš„æŸ¥è¯¢è¯­å¥ã€‚
"""
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_query},
                ]
            else:
                # éœ€è¦æ•°æ®åº“æŸ¥è¯¢çš„æƒ…å†µ
                parsed_query = self.query_parser.parse_query(user_query)
                query_type = "statistics" if "ç»Ÿè®¡" in user_query else "search"
                retrieved_data = self.retrieval_service.search_by_conditions(
                    parsed_query, 500
                )

                # æ„å»ºä¸Šä¸‹æ–‡
                if query_type == "statistics":
                    stats = None
                    if isinstance(parsed_query, QueryConditions):
                        stats = self.retrieval_service.get_statistics(parsed_query)
                    context_text = self.context_builder.build_statistics_context(
                        user_query, retrieved_data, stats
                    )
                else:
                    context_text = self.context_builder.build_search_context(
                        user_query, retrieved_data
                    )
                print(context_text)
                messages = [
                    {"role": "system", "content": context_text},
                    {"role": "user", "content": user_query},
                ]
                temperature = 0.3

            return {
                "success": True,
                "messages": messages,
                "temperature": temperature,
                "intent_result": intent_result,
            }

        except Exception as e:
            logger.error(f"[stream] å¤„ç†æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "success": False,
                "response": f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "messages": [],
                "temperature": 0.7,
            }
